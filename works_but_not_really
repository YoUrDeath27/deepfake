import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # Force TensorFlow to use CPU

import io
import base64
import pandas as pd

from flask import Flask, request, render_template_string, render_template, redirect
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import librosa
import numpy as np
import matplotlib.pyplot as plt
from pyngrok import ngrok
import cv2
from werkzeug.utils import secure_filename
import torch
from torch.utils.model_zoo import load_url
from PIL import Image
from keras.preprocessing.image import load_img,img_to_array
import lime
from lime import lime_image
from skimage.segmentation import mark_boundaries
import sys
import resampy

# Add custom modules to the path
sys.path.append('..')
from blazeface import FaceExtractor, BlazeFace
from architectures import fornet, weights
from isplutils import utils

# Initialize Flask app
app = Flask(__name__)

# Step 2: Load Your Model and Weights
net_model = 'EfficientNetAutoAttB4'
train_db = 'DFDC'

device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')
face_policy = 'scale'
face_size = 224

model_url = weights.weight_url[f'{net_model}_{train_db}']
net = getattr(fornet, net_model)().eval().to(device)
net.load_state_dict(load_url(model_url, map_location=device, check_hash=True))

transf = utils.get_transformer(face_policy, face_size, net.get_normalizer(), train=False)

facedet = BlazeFace().to(device)
facedet.load_weights("../blazeface/blazeface.pth")
facedet.load_anchors("../blazeface/anchors.npy")
face_extractor = FaceExtractor(facedet=facedet)

# Load pre-trained VGG16 model for binary classification
base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
model_audio = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification
])
model_audio.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

class_names = ['fake', 'real',]  # Ensure this matches your model's output classes

model_text = load_model('/content/deepfake/text.h5')
model_text.summary()

# Specify the upload folder
upload = '/content/uploads'
app.config['UPLOAD_FOLDER'] = upload

# Create the upload folder if it doesn't exist
if not os.path.exists(app.config['UPLOAD_FOLDER']):
    os.makedirs(app.config['UPLOAD_FOLDER'])

# Allowed file extensions
app.config['ALLOWED_EXTENSIONS'] = {'mp4', 'avi', 'mov', 'jpg', 'jpeg', 'png', 'gif', 'wav', 'mp3'}

# Helper function to check file extensions
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

# Route to display the upload form
def response(response):
  return '''
    <!DOCTYPE html>
    <html lang="en">
    <head>

        <title>Insert Text</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                text-align: center;
                background-color: #1c1b1b;
                color: white;
            }
            button {
                padding: 10px 20px;
                margin: 10px;
                font-size: 16px;
                cursor: pointer;
                background-color: #1e1e1e;
                color: #ffffff;
                border: 1px solid #ffffff;
            }
            button:hover {
                background-color: #ffffff;
                color: #121212;
            }
        </style>
    </head>
    <body>
        <h1> Result: </h1>
        <h3> ''' + str(response) + ''' </h3>
        <button onclick="window.location.href='/'">Upload another file/ text to evaluate</button>
    </body>
    </html>
   '''

def response_audio(response, image):
  return '''
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <title>Insert Text</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                text-align: center;
                background-color: #1c1b1b;
                color: white;
            }
            button {
                padding: 10px 20px;
                margin: 10px;
                font-size: 16px;
                cursor: pointer;
                background-color: #1e1e1e;
                color: #ffffff;
                border: 1px solid #ffffff;
            }
            button:hover {
                background-color: #ffffff;
                color: #121212;
            }
        </style>
    </head>
    <body>
        <h1>Audio File Analysis</h1>
            <h2> ''' + response + '''</h2>
            <h2>LIME Explanation</h2>
            <img src="data:image/png;base64, ''' + image_to_base64(image) + ''' " width="500"/>
            <br>
            <br>
        <button onclick="window.location.href='/'">Upload another file/ text to evaluate</button>
    </body>
    </html>
  '''

@app.route('/')
def index():
    return '''
    <!DOCTYPE html>
    <html lang="en">
    <head>

        <title>Deepfake Detection</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                text-align: center;

                background-color: #121212;
                color: #ffffff;
            }
            h1 {
                margin-bottom: 30px;
            }
            button {
                padding: 10px 20px;
                margin: 10px;
                font-size: 16px;
                cursor: pointer;
                background-color: #1e1e1e;
                color: #ffffff;
                border: 1px solid #ffffff;


            }
            button:hover {
                background-color: #ffffff;
                color: #121212;
            }
        </style>
    </head>
    <body>
        <h1>Deepfake Detection</h1>
        <button onclick="window.location.href='text'">Insert Text</button>
        <button onclick="window.location.href='audio'">Upload Audio</button>
        <button onclick="window.location.href='image'">Upload Video / Image</button>
    </body>
    </html>
    '''

@app.route("/image")
def image():
  return '''
    <!DOCTYPE html>
    <html lang="en">
    <head>

        <title>Upload Video/Image</title>
        <style>
            body {
                display: flex;
                background-color: #151617;
                color: #e8e6e3;
                justify-content: center;
            }
            form{
                  height: 200px;
                  display: flex;
                  background-color: #1f1f1f;
                  width: 300px;
                  flex-direction: column;
                  justify-content: center;
                  align-items: center;
            }
            input {
              width: 180px;
            }
        </style>
    </head>
    <body>
      <form action="/upload" method="post" enctype="multipart/form-data">
        <label for="video">Insert Video/Image:</label> </h1>
        <input type="file" name="file" accept="image/*,video/*,.gif" required>
        <br>
        <br>
        <input type="submit">
      </form>
    </body>
    </html>
  '''

@app.route("/audio")
def audio():
  return '''
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Upload Audio</title>
        <style>
            body {
                display: flex;
                background-color: #151617;
                color: #e8e6e3;
                justify-content: center;
            }
            form{
                  height: 200px;
                  display: flex;
                  background-color: #1f1f1f;
                  width: 300px;
                  flex-direction: column;
                  justify-content: center;
                  align-items: center;
            }
            input {
              width: 180px;
            }
        </style>
    </head>
    <body>
      <form action="/upload" method="post" enctype="multipart/form-data">
        <label for="audio">Insert Audio:</label> </h1>
        <input type="file" name="file" accept=".wav" required>
        <br>
        <br>
        <input type="submit">
      </form>
    </body>
    </html>

  '''
@app.route('/text')
def text():
  return '''
    <!DOCTYPE html>
    <html lang="en">
    <head>

        <title>Insert Text</title>
        <style>
            body {
                display: flex;
                background-color: #151617;
                color: #e8e6e3;
                justify-content: center;
            }
            form{
                  height: 200px;
                  display: flex;
                  background-color: #1f1f1f;
                  width: 300px;
                  flex-direction: column;
                  justify-content: center;
                  align-items: center;
            }
            input {
              width: 180px;
            }
        </style>
    </head>
    <body>
      <form action="/upload" method="post" enctype="multipart/form-data">
        <label for="text">Insert Text:</label> </h1>
        <input type="text" id="text" name="text">
        <br>
        <br>
        <input type="submit">
      </form>
    </body>
    </html>
  '''
# Route to handle the file upload and processing
@app.route('/upload', methods=['POST'])
def upload_file():
    # Handle text submission
    if 'text' in request.form:
        text =  predict_text(request.form['text'])
        if len(text) == 0:
          return response("No text in here")
        return response([text])

    # Handle file upload (existing code)
    if 'file' not in request.files:
        return "No file part"
    file = request.files['file']
    if file.filename == '':
        return "No selected file"

    if file and allowed_file (file.filename):
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)

        # Process the file based on its type
        if file.filename.endswith(('.mp4', '.avi', '.mov')):
            result = process(filepath)
        elif file.filename.endswith(('.jpg', '.jpeg', '.png', '.gif')):
            result = process(filepath)
        elif file.filename.endswith(('.wav')):
            result = detect_audio_deepfake(filepath)
            return response(result)
    return "Invalid file type or no file selected."

def predict_text(text):

  X_input = [text]

  # Tokenization (use the same tokenizer used during training)
  tokenizer = Tokenizer(num_words=5000, lower=True)
  tokenizer.fit_on_texts(X_input)  # Fit on input data (or use the same tokenizer from training)
  X_input_seq = tokenizer.texts_to_sequences(X_input)
  X_input_pad = pad_sequences(X_input_seq, padding='post', maxlen=100)

  # Step 6: Make predictions
  predictions = model_text.predict(X_input_pad)
  predicted_classes = (predictions > 0.5).astype(int)  # Convert probabilities to binary class

  # Print incorrect predictions
  for i, title in enumerate(X_input):
    print(f"Title: {title}")
    print(f"Predicted Class: {'Fake' if predicted_classes[i][0] == 1 else 'Real'}")
    print(f"Prediction Confidence: {predictions[i][0]:.4f}")
    print("-" * 50)

    return (f"Predicted Class: {'Fake' if predicted_classes[i][0] == 1 else 'Real'}")

def detect_audio_deepfake(audio_path):
    # Load the dataset (replace 'dataset.csv' with your actual dataset path)
    dataset = pd.read_csv('/content/deepfake/dataset.csv')
    num_mfcc = 100
    num_mels = 128
    num_chroma = 50

    # Process audio file
    X, sample_rate = librosa.load(audio_path, res_type='kaiser_fast')
    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=num_mfcc).T, axis=0)
    mel_spectrogram = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate, n_mels=num_mels).T, axis=0)
    chroma_features = np.mean(librosa.feature.chroma_stft(y=X, sr=sample_rate, n_chroma=num_chroma).T, axis=0)
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=X).T, axis=0)
    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=X, sr=sample_rate).T, axis=0)
    flatness = np.mean(librosa.feature.spectral_flatness(y=X).T, axis=0)
    features = np.concatenate((mfccs, mel_spectrogram, chroma_features, zcr, spectral_centroid, flatness))

    # Find closest match in the dataset
    distances = np.linalg.norm(dataset.iloc[:, :-1] - features, axis=1)
    closest_match_idx = np.argmin(distances)
    closest_match_label = dataset.iloc[closest_match_idx, -1]
    total_distance = np.sum(distances)
    closest_match_prob = 1 - (distances[closest_match_idx] / total_distance)
    closest_match_prob_percentage = "{:.3f}".format(closest_match_prob * 100)

    # Generate result
    if closest_match_label == 'deepfake':
        result = f"Result: Fake with {closest_match_prob_percentage}% confidence"
    else:
        result = f"Result: Real with {closest_match_prob_percentage}% confidence"

    return result

# Helper function to convert images to base64 encoding for display
def image_to_base64(fig):
    import io
    from base64 import b64encode
    buf = io.BytesIO()
    fig.savefig(buf, format="png")
    buf.seek(0)
    return b64encode(buf.read()).decode('utf-8')

def process(detect_path):
    cam = cv2.VideoCapture(detect_path)
    last_frame = None

    while True:
        ret, frame = cam.read()
        if ret:
            last_frame = frame
        else:
            break

    cam.release()

    if last_frame is not None:
        return analyze_frame(last_frame)
    else:
        return "No frames detected"

def analyze_frame(frame):
    # Check if the frame is empty
    if frame is None or frame.size == 0:
        return "Error: The frame is empty."

    # Convert BGR to RGB
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Extract face
    image_face = face_extractor.process_image(img=frame_rgb)
    if 'faces' in image_face and len(image_face['faces']) > 0:
        image_face = image_face['faces'][0]
    else:
        return "No face detected"

    # Prepare the face for prediction
    faces_t = torch.stack([transf(image=im)['image'] for im in [image_face]])

    with torch.no_grad():
        faces_pred = torch.sigmoid(net(faces_t.to(device))).cpu().numpy().flatten()

    # Print scores
    score = faces_pred[0]
    result = "Image is fake" if score > 0.5 else "Image is real"
    return f'Score for face: {score:.5f}. {result}'

# Start the Flask app
if __name__ == '__main__':
    # Start ngrok
    ngrok.set_auth_token('2rTV3LPAsyBUa3eHsXv7V6kpGb1_25Tcwtwofie8Vi2rh1DFb')  # Replace with your Ngrok token
    public_url = ngrok.connect(5000)
    print(f' * ngrok tunnel "{public_url}" ')

    app.run(port=5000)
